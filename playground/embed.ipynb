{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2545919b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "from papersys.const import BASE_DIR\n",
    "\n",
    "load_dotenv(BASE_DIR / \".env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e934c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function litellm.main.embedding(model, input=[], dimensions: Optional[int] = None, encoding_format: Optional[str] = None, timeout=600, api_base: Optional[str] = None, api_version: Optional[str] = None, api_key: Optional[str] = None, api_type: Optional[str] = None, caching: bool = False, user: Optional[str] = None, custom_llm_provider=None, litellm_call_id=None, logger_fn=None, **kwargs) -> Union[litellm.types.utils.EmbeddingResponse, Coroutine[Any, Any, litellm.types.utils.EmbeddingResponse]]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b5831a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_cost_per_token': 1.5e-07,\n",
       " 'litellm_provider': 'vertex_ai-embedding-models',\n",
       " 'max_input_tokens': 2048,\n",
       " 'max_tokens': 2048,\n",
       " 'mode': 'embedding',\n",
       " 'output_cost_per_token': 0,\n",
       " 'output_vector_size': 3072,\n",
       " 'source': 'https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.model_cost['gemini-embedding-001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14898045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹: gemini-embedding-001\n",
      "åµŒå…¥ç»´åº¦: 3072\n",
      "ç”Ÿæˆäº† 2 ä¸ªåµŒå…¥å‘é‡\n",
      "\n",
      "ç¬¬ä¸€ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆå‰10ç»´ï¼‰: [-0.004167554, 0.010072836, 0.008006427, -0.085819386, 0.0007920959, 0.006507263, 0.019792652, 0.010993583, 0.006896781, 0.004768953]\n",
      "ç¬¬äºŒä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆå‰10ç»´ï¼‰: [-0.0039723944, 0.0037366473, 0.010403045, -0.06857629, -0.009790573, -0.011662395, -0.003661297, 0.018717255, 0.015728533, -0.0025070773]\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ litellm.embedding() è¿›è¡Œæ‰¹é‡ embedding\n",
    "response = litellm.embedding(\n",
    "    model=\"gemini/gemini-embedding-001\",  # æŒ‡å®šä½¿ç”¨ Gemini embedding æ¨¡å‹\n",
    "    input=[\"I love programming.\", \"Python is great for data science.\"]  # æ‰¹é‡è¾“å…¥æ–‡æœ¬\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹ç»“æœ\n",
    "print(f\"æ¨¡å‹: {response.model}\")\n",
    "print(f\"åµŒå…¥ç»´åº¦: {len(response.data[0].embedding)}\")\n",
    "print(f\"ç”Ÿæˆäº† {len(response.data)} ä¸ªåµŒå…¥å‘é‡\")\n",
    "print(f\"\\nç¬¬ä¸€ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆå‰10ç»´ï¼‰: {response.data[0].embedding[:10]}\")\n",
    "print(f\"ç¬¬äºŒä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆå‰10ç»´ï¼‰: {response.data[1].embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e582a081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰¹é‡å¤„ç†äº† 4 ä¸ªæ–‡æœ¬\n",
      "æ¯ä¸ªåµŒå…¥å‘é‡çš„ç»´åº¦: 3072\n",
      "åµŒå…¥å‘é‡åˆ—è¡¨å½¢çŠ¶: 4 x 3072\n"
     ]
    }
   ],
   "source": [
    "# æå–åµŒå…¥å‘é‡æ•°æ®\n",
    "embeddings = [item.embedding for item in response.data]\n",
    "\n",
    "# æˆ–è€…å¦‚æœéœ€è¦å¤„ç†æ›´å¤šæ–‡æœ¬ï¼Œå¯ä»¥è¿™æ ·æ‰¹é‡å¤„ç†\n",
    "texts = [\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"I enjoy coding in Python.\",\n",
    "    \"Data science combines statistics and programming.\",\n",
    "    \"AI is transforming the world.\"\n",
    "]\n",
    "\n",
    "batch_response = litellm.embehdding(\n",
    "    model=\"gemini/gemini-embedding-001\",\n",
    "    input=texts\n",
    ")\n",
    "\n",
    "print(f\"æ‰¹é‡å¤„ç†äº† {len(batch_response.data)} ä¸ªæ–‡æœ¬\")\n",
    "print(f\"æ¯ä¸ªåµŒå…¥å‘é‡çš„ç»´åº¦: {len(batch_response.data[0].embedding)}\")\n",
    "\n",
    "# è·å–æ‰€æœ‰åµŒå…¥å‘é‡\n",
    "all_embeddings = [item.embedding for item in batch_response.data]\n",
    "print(f\"åµŒå…¥å‘é‡åˆ—è¡¨å½¢çŠ¶: {len(all_embeddings)} x {len(all_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d6dac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google_batch_embeddings æ¨¡å— ===\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_check_custom_proxy', '_credentials', '_credentials_from_authorized_user', '_credentials_from_default_auth', '_credentials_from_identity_pool', '_credentials_from_identity_pool_with_aws', '_credentials_from_service_account', '_credentials_project_mapping', '_ensure_access_token', '_ensure_access_token_async', '_get_token_and_url', '_handle_reauthentication', 'access_token', 'async_batch_embeddings', 'async_completion', 'async_handler', 'async_streaming', 'batch_embeddings', 'completion', 'create_vertex_url', 'get_access_token', 'get_api_base', 'get_complete_vertex_url', 'get_default_vertex_location', 'get_vertex_ai_credentials', 'get_vertex_ai_location', 'get_vertex_ai_project', 'get_vertex_region', 'is_using_v1beta1_features', 'load_auth', 'project_id', 'refresh_auth', 'refresh_token', 'set_headers']\n",
      "\n",
      "=== batch_embeddings å‡½æ•°ç­¾å ===\n",
      "(model: str, input: Union[str, List[str]], print_verbose, model_response: litellm.types.utils.EmbeddingResponse, custom_llm_provider: Literal['gemini', 'vertex_ai'], optional_params: dict, logging_obj: Any, api_key: Optional[str] = None, api_base: Optional[str] = None, encoding=None, vertex_project=None, vertex_location=None, vertex_credentials=None, aembedding: Optional[bool] = False, timeout=300, client=None) -> litellm.types.utils.EmbeddingResponse\n",
      "\n",
      "=== batch_embeddings æ–‡æ¡£ ===\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# è®©æˆ‘ä»¬å…ˆæ£€æŸ¥ litellm ä¸­å…³äº batch embedding çš„åŠŸèƒ½\n",
    "import inspect\n",
    "\n",
    "# æŸ¥çœ‹ google_batch_embeddings æ¨¡å—\n",
    "print(\"=== google_batch_embeddings æ¨¡å— ===\")\n",
    "print(dir(litellm.google_batch_embeddings))\n",
    "print()\n",
    "\n",
    "# æŸ¥çœ‹ batch_embeddings å‡½æ•°çš„ç­¾å\n",
    "print(\"=== batch_embeddings å‡½æ•°ç­¾å ===\")\n",
    "print(inspect.signature(litellm.google_batch_embeddings.batch_embeddings))\n",
    "print()\n",
    "\n",
    "# æŸ¥çœ‹å‡½æ•°æ–‡æ¡£\n",
    "print(\"=== batch_embeddings æ–‡æ¡£ ===\")\n",
    "print(litellm.google_batch_embeddings.batch_embeddings.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb52739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== litellm.embedding å‡½æ•°ç­¾å ===\n",
      "(model, input=[], dimensions: Optional[int] = None, encoding_format: Optional[str] = None, timeout=600, api_base: Optional[str] = None, api_version: Optional[str] = None, api_key: Optional[str] = None, api_type: Optional[str] = None, caching: bool = False, user: Optional[str] = None, custom_llm_provider=None, litellm_call_id=None, logger_fn=None, **kwargs) -> Union[litellm.types.utils.EmbeddingResponse, Coroutine[Any, Any, litellm.types.utils.EmbeddingResponse]]\n",
      "\n",
      "=== litellm ä¸­åŒ…å« 'batch' çš„å±æ€§ ===\n",
      "['AzureBatchesAPI', 'Batch', 'BatchJobStatus', 'CancelBatchRequest', 'CreateBatchRequest', 'DEFAULT_BATCH_SIZE', 'GoogleBatchEmbeddings', 'ListBatchRequest', 'LiteLLMBatch', 'LiteLLMBatchCreateRequest', 'OpenAIBatchesAPI', 'RetrieveBatchRequest', 'VertexAIBatchPrediction', 'acancel_batch', 'acreate_batch', 'alist_batches', 'aretrieve_batch', 'argilla_batch_size', 'azure_batches_instance', 'batch_completion', 'batch_completion_models', 'batch_completion_models_all_responses', 'batches', 'cancel_batch', 'create_batch', 'custom_batch_logger', 'default_redis_batch_cache_expiry', 'enable_loadbalancing_on_batch_endpoints', 'google_batch_embeddings', 'langsmith_batch_size', 'list_batches', 'openai_batches_instance', 'retrieve_batch', 'vertex_ai_batches_instance']\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥ litellm.embedding æ˜¯å¦æ”¯æŒ batch å‚æ•°\n",
    "print(\"=== litellm.embedding å‡½æ•°ç­¾å ===\")\n",
    "print(inspect.signature(litellm.embedding))\n",
    "print()\n",
    "\n",
    "# æŸ¥çœ‹æ˜¯å¦æœ‰å…¶ä»– batch ç›¸å…³çš„åŠŸèƒ½\n",
    "print(\"=== litellm ä¸­åŒ…å« 'batch' çš„å±æ€§ ===\")\n",
    "batch_attrs = [attr for attr in dir(litellm) if 'batch' in attr.lower()]\n",
    "print(batch_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde216da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== batch_completion å‡½æ•° ===\n",
      "(model: str, messages: List = [], functions: Optional[List] = None, function_call: Optional[str] = None, temperature: Optional[float] = None, top_p: Optional[float] = None, n: Optional[int] = None, stream: Optional[bool] = None, stop=None, max_tokens: Optional[int] = None, presence_penalty: Optional[float] = None, frequency_penalty: Optional[float] = None, logit_bias: Optional[dict] = None, user: Optional[str] = None, deployment_id=None, request_timeout: Optional[int] = None, timeout: Optional[int] = 600, max_workers: Optional[int] = 100, **kwargs)\n",
      "\n",
      "\n",
      "    Batch litellm.completion function for a given model.\n",
      "\n",
      "    Args:\n",
      "        model (str): The model to use for generating completions.\n",
      "        messages (List, optional): List of messages to use as input for generating completions. Defaults to [].\n",
      "        functions (List, optional): List of functions to use as input for generating completions. Defaults to [].\n",
      "        function_call (str, optional): The function call to use as input for generating completions. Defaults to \"\".\n",
      "        temperature (float, optional): The temperature parameter for generating completions. Defaults to None.\n",
      "        top_p (float, optional): The top-p parameter for generating completions. Defaults to None.\n",
      "        n (int, optional): The number of completions to generate. Defaults to None.\n",
      "        stream (bool, optional): Whether to stream completions or not. Defaults to None.\n",
      "        stop (optional): The stop parameter for generating completions. Defaults to None.\n",
      "        max_tokens (float, optional): The maximum number of tokens to generate. Defaults to None.\n",
      "        presence_penalty (float, optional): The presence penalty for generating completions. Defaults to None.\n",
      "        frequency_penalty (float, optional): The frequency penalty for generating completions. Defaults to None.\n",
      "        logit_bias (dict, optional): The logit bias for generating completions. Defaults to {}.\n",
      "        user (str, optional): The user string for generating completions. Defaults to \"\".\n",
      "        deployment_id (optional): The deployment ID for generating completions. Defaults to None.\n",
      "        request_timeout (int, optional): The request timeout for generating completions. Defaults to None.\n",
      "        max_workers (int,optional): The maximum number of threads to use for parallel processing.\n",
      "\n",
      "    Returns:\n",
      "        list: A list of completion results.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹ batch_completion çš„ç­¾åå’Œæ–‡æ¡£\n",
    "print(\"=== batch_completion å‡½æ•° ===\")\n",
    "print(inspect.signature(litellm.batch_completion))\n",
    "print()\n",
    "print(litellm.batch_completion.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dca4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:35 - LiteLLM:WARNING\u001b[0m: utils.py:550 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯•è°ƒç”¨ litellm.embedding æ—¶çš„æ—¥å¿—ï¼š\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n"
     ]
    }
   ],
   "source": [
    "# è®©æˆ‘æµ‹è¯•ä¸€ä¸‹ï¼šå½“æˆ‘ä»¬ä¼ é€’å¤šä¸ªæ–‡æœ¬ç»™ litellm.embedding æ—¶ï¼Œ\n",
    "# å®ƒå®é™…ä¸Šè°ƒç”¨çš„æ˜¯ä»€ä¹ˆ API\n",
    "\n",
    "# é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç›´æ¥æŸ¥çœ‹ Google AI çš„ embedding API\n",
    "# Google AI Studio çš„ Gemini API æœ‰ä¸¤ä¸ªç«¯ç‚¹ï¼š\n",
    "# 1. embedContent - å•ä¸ªæ–‡æœ¬\n",
    "# 2. batchEmbedContents - æ‰¹é‡æ–‡æœ¬\n",
    "\n",
    "# è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ litellm çš„è¯¦ç»†æ—¥å¿—æ¥æŸ¥çœ‹å®é™…è°ƒç”¨\n",
    "litellm.set_verbose = True\n",
    "\n",
    "print(\"æµ‹è¯•è°ƒç”¨ litellm.embedding æ—¶çš„æ—¥å¿—ï¼š\")\n",
    "test_response = litellm.embedding(\n",
    "    model=\"gemini/gemini-embedding-001\",\n",
    "    input=[\"test 1\", \"test 2\", \"test 3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744961b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google.generativeai.embed_content ç›¸å…³åŠŸèƒ½ ===\n",
      "['embed_content', 'embed_content_async']\n",
      "\n",
      "è¿”å›çš„åµŒå…¥æ•°é‡: 3\n",
      "ç¬¬ä¸€ä¸ªåµŒå…¥çš„ç»´åº¦: 768\n",
      "ç»“æœç±»å‹: <class 'dict'>\n",
      "ç»“æœé”®: dict_keys(['embedding'])\n"
     ]
    }
   ],
   "source": [
    "# å…³é—­ verbose æ¨¡å¼\n",
    "litellm.set_verbose = False\n",
    "\n",
    "# è®©æˆ‘ç›´æ¥ä½¿ç”¨ Google çš„ generativeai åº“æ¥æµ‹è¯• batch embedding\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# é…ç½® API key\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# æŸ¥çœ‹å¯ç”¨çš„æ–¹æ³•\n",
    "print(\"=== google.generativeai.embed_content ç›¸å…³åŠŸèƒ½ ===\")\n",
    "print([attr for attr in dir(genai) if 'embed' in attr.lower()])\n",
    "print()\n",
    "\n",
    "# æµ‹è¯• batch embedding\n",
    "texts = [\"text 1\", \"text 2\", \"text 3\"]\n",
    "result = genai.embed_content(\n",
    "    model=\"models/embedding-001\",\n",
    "    content=texts,\n",
    "    task_type=\"retrieval_document\"\n",
    ")\n",
    "\n",
    "print(f\"è¿”å›çš„åµŒå…¥æ•°é‡: {len(result['embedding'])}\")\n",
    "print(f\"ç¬¬ä¸€ä¸ªåµŒå…¥çš„ç»´åº¦: {len(result['embedding'][0]) if isinstance(result['embedding'][0], list) else 'scalar'}\")\n",
    "print(f\"ç»“æœç±»å‹: {type(result)}\")\n",
    "print(f\"ç»“æœé”®: {result.keys() if hasattr(result, 'keys') else 'not a dict'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6469f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google.generativeai æ‰€æœ‰å…¬å¼€æ–¹æ³• ===\n",
      "  - embed_content\n",
      "  - embed_content_async\n",
      "\n",
      "æ£€æŸ¥ batch_embed_contents:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Google AI çš„ embed_content å®é™…ä¸Šæ”¯æŒæ‰¹é‡è¾“å…¥\n",
    "# ä½†è®©æˆ‘æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰ä¸“é—¨çš„ batch_embed_contents æ–¹æ³•\n",
    "\n",
    "# æ£€æŸ¥ genai æ¨¡å—çš„æ‰€æœ‰æ–¹æ³•\n",
    "print(\"=== google.generativeai æ‰€æœ‰å…¬å¼€æ–¹æ³• ===\")\n",
    "public_methods = [attr for attr in dir(genai) if not attr.startswith('_')]\n",
    "for method in public_methods:\n",
    "    if 'batch' in method.lower() or 'embed' in method.lower():\n",
    "        print(f\"  - {method}\")\n",
    "print()\n",
    "\n",
    "# å®é™…ä¸Šï¼Œæ ¹æ® Google AI çš„æ–‡æ¡£ï¼š\n",
    "# embed_content å·²ç»æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œå½“ä½ ä¼ å…¥åˆ—è¡¨æ—¶ä¼šè‡ªåŠ¨æ‰¹å¤„ç†\n",
    "# ä½†è¦ä½¿ç”¨ batch APIï¼ˆæ›´ä¾¿å®œï¼‰ï¼Œéœ€è¦ä½¿ç”¨ batch_embed_contents\n",
    "\n",
    "# è®©æˆ‘æŸ¥æ‰¾æ˜¯å¦æœ‰è¿™ä¸ªæ–¹æ³•\n",
    "try:\n",
    "    print(\"æ£€æŸ¥ batch_embed_contents:\")\n",
    "    print(hasattr(genai, 'batch_embed_contents'))\n",
    "    if hasattr(genai, 'batch_embed_contents'):\n",
    "        print(genai.batch_embed_contents)\n",
    "except Exception as e:\n",
    "    print(f\"é”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29366f1e",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ Batch Embedding APIï¼ˆä»·æ ¼å‡åŠï¼ï¼‰\n",
    "\n",
    "æ ¹æ® Google æ–‡æ¡£ï¼ŒBatch API æä¾›ï¼š\n",
    "- **ä»·æ ¼ï¼šæ­£å¸¸ä»·æ ¼çš„ 50%** ğŸ’°\n",
    "- **å¤„ç†æ–¹å¼ï¼šå¼‚æ­¥å¤„ç†**\n",
    "- **è¿”å›æ—¶é—´ï¼šç›®æ ‡ 24 å°æ—¶å†…ï¼Œé€šå¸¸æ›´å¿«**\n",
    "- **é€‚ç”¨åœºæ™¯ï¼šå¤§é‡æ•°æ®é¢„å¤„ç†ã€éç´§æ€¥ä»»åŠ¡**\n",
    "\n",
    "ä¸‹é¢æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Google çš„æ–° SDK (`google-genai`) æ¥è°ƒç”¨ Batch Embedding APIï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105da511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡äº† 5 ä¸ª embedding è¯·æ±‚\n",
      "åˆ›å»º batch job...\n"
     ]
    }
   ],
   "source": [
    "# æ–¹æ³•1ï¼šä½¿ç”¨ inline requestsï¼ˆé€‚åˆå°‘é‡è¯·æ±‚ï¼‰\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# å‡†å¤‡è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨\n",
    "texts_to_embed = [\n",
    "    \"Machine learning is transforming technology.\",\n",
    "    \"Python is excellent for data science.\",\n",
    "    \"Natural language processing enables AI to understand text.\",\n",
    "    \"Deep learning models require large datasets.\",\n",
    "    \"Transfer learning reduces training time.\"\n",
    "]\n",
    "\n",
    "# åˆ›å»º inline embedding è¯·æ±‚\n",
    "inline_requests = []\n",
    "for idx, text in enumerate(texts_to_embed):\n",
    "    request = {\n",
    "        'content': text,\n",
    "        'task_type': 'RETRIEVAL_DOCUMENT',  # æŒ‡å®šä»»åŠ¡ç±»å‹\n",
    "    }\n",
    "    inline_requests.append(request)\n",
    "\n",
    "print(f\"å‡†å¤‡äº† {len(inline_requests)} ä¸ª embedding è¯·æ±‚\")\n",
    "print(\"åˆ›å»º batch job...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "390715ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== batches.create_embeddings ç­¾å ===\n",
      "(*, model: str, src: Union[google.genai.types.EmbeddingsBatchJobSource, google.genai.types.EmbeddingsBatchJobSourceDict], config: Union[google.genai.types.CreateEmbeddingsBatchJobConfig, google.genai.types.CreateEmbeddingsBatchJobConfigDict, NoneType] = None) -> google.genai.types.BatchJob\n",
      "\n",
      "=== EmbeddingsBatchJobSource çš„å­—æ®µ ===\n",
      "{'file_name': typing.Optional[str], 'inlined_requests': typing.Optional[google.genai.types.EmbedContentBatch]}\n"
     ]
    }
   ],
   "source": [
    "# è®©æˆ‘å…ˆæŸ¥çœ‹ batches.create_embeddings çš„æ­£ç¡®ç”¨æ³•\n",
    "import inspect\n",
    "\n",
    "print(\"=== batches.create_embeddings ç­¾å ===\")\n",
    "print(inspect.signature(client.batches.create_embeddings))\n",
    "print()\n",
    "\n",
    "# æŸ¥çœ‹éœ€è¦ä»€ä¹ˆç±»å‹çš„æ•°æ®\n",
    "from google.genai import types\n",
    "print(\"=== EmbeddingsBatchJobSource çš„å­—æ®µ ===\")\n",
    "print(types.EmbeddingsBatchJobSource.__annotations__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e25985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EmbedContentBatch çš„å­—æ®µ ===\n",
      "{'contents': typing.Union[google.genai.types.Content, str, PIL.Image.Image, google.genai.types.File, google.genai.types.Part, list[typing.Union[str, PIL.Image.Image, google.genai.types.File, google.genai.types.Part]], list[typing.Union[google.genai.types.Content, str, PIL.Image.Image, google.genai.types.File, google.genai.types.Part, list[typing.Union[str, PIL.Image.Image, google.genai.types.File, google.genai.types.Part]]]], NoneType], 'config': typing.Optional[google.genai.types.EmbedContentConfig]}\n",
      "\n",
      "=== EmbedContentBatch æ–‡æ¡£ ===\n",
      "Parameters for the embed_content method.\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹ EmbedContentBatch çš„ç»“æ„\n",
    "print(\"=== EmbedContentBatch çš„å­—æ®µ ===\")\n",
    "print(types.EmbedContentBatch.__annotations__)\n",
    "print()\n",
    "\n",
    "# æŸ¥çœ‹ EmbedContentBatch çš„æ–‡æ¡£\n",
    "print(\"=== EmbedContentBatch æ–‡æ¡£ ===\")\n",
    "print(types.EmbedContentBatch.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34676c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17058/2424147294.py:5: ExperimentalWarning: batches.create_embeddings() is experimental and may change without notice.\n",
      "  batch_job = client.batches.create_embeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Batch job åˆ›å»ºæˆåŠŸï¼\n",
      "Job name: batches/ms9ssfwsac710gijdq6sft5jzz48jebfu50d\n",
      "Job state: JOB_STATE_PENDING\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BatchJob' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJob name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_job.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJob state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_job.state.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDisplay name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbatch_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m.display_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mæ³¨æ„ï¼šè¿™æ˜¯å¼‚æ­¥å¤„ç†ï¼Œä»·æ ¼æ˜¯æ­£å¸¸ä»·æ ¼çš„ 50%ï¼\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papersys/.venv/lib/python3.12/site-packages/pydantic/main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'BatchJob' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "# æ­£ç¡®çš„æ–¹å¼ï¼šä½¿ç”¨ EmbedContentBatch\n",
    "from google.genai import types\n",
    "\n",
    "# åˆ›å»º batch embedding job\n",
    "batch_job = client.batches.create_embeddings(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    src={\n",
    "        'inlined_requests': types.EmbedContentBatch(\n",
    "            contents=texts_to_embed,  # ç›´æ¥ä¼ å…¥æ–‡æœ¬åˆ—è¡¨\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type='RETRIEVAL_DOCUMENT',  # æŒ‡å®šä»»åŠ¡ç±»å‹\n",
    "                output_dimensionality=768  # å¯é€‰ï¼šæŒ‡å®šç»´åº¦ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´\n",
    "            )\n",
    "        )\n",
    "    },\n",
    "    config={'display_name': \"test-batch-embeddings\"},\n",
    ")\n",
    "\n",
    "print(f\"âœ… Batch job åˆ›å»ºæˆåŠŸï¼\")\n",
    "print(f\"Job name: {batch_job.name}\")\n",
    "print(f\"Job state: {batch_job.state.name}\")\n",
    "print(f\"\\nğŸ’° ä»·æ ¼ä¼˜åŠ¿ï¼šBatch API ä»·æ ¼æ˜¯æ­£å¸¸ä»·æ ¼çš„ 50%ï¼\")\n",
    "print(f\"â±ï¸  å¤„ç†æ—¶é—´ï¼šå¼‚æ­¥å¤„ç†ï¼Œé€šå¸¸å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶å†…å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5997a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨æ£€æŸ¥ job çŠ¶æ€: batches/ms9ssfwsac710gijdq6sft5jzz48jebfu50d\n",
      "\n",
      "[1] å½“å‰çŠ¶æ€: JOB_STATE_PENDING\n",
      "   ç­‰å¾… 5 ç§’åå†æ¬¡æ£€æŸ¥...\n",
      "[2] å½“å‰çŠ¶æ€: JOB_STATE_SUCCEEDED\n",
      "\n",
      "ğŸ‰ Job å®Œæˆï¼æœ€ç»ˆçŠ¶æ€: JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥ job çŠ¶æ€\n",
    "import time\n",
    "\n",
    "job_name = batch_job.name\n",
    "print(f\"æ­£åœ¨æ£€æŸ¥ job çŠ¶æ€: {job_name}\\n\")\n",
    "\n",
    "# è½®è¯¢çŠ¶æ€ç›´åˆ°å®Œæˆ\n",
    "completed_states = {\n",
    "    'JOB_STATE_SUCCEEDED',\n",
    "    'JOB_STATE_FAILED',\n",
    "    'JOB_STATE_CANCELLED',\n",
    "    'JOB_STATE_EXPIRED',\n",
    "}\n",
    "\n",
    "max_polls = 10  # æœ€å¤šè½®è¯¢ 10 æ¬¡ï¼ˆæ¼”ç¤ºç”¨ï¼‰\n",
    "poll_count = 0\n",
    "\n",
    "while poll_count < max_polls:\n",
    "    batch_job = client.batches.get(name=job_name)\n",
    "    current_state = batch_job.state.name\n",
    "    print(f\"[{poll_count + 1}] å½“å‰çŠ¶æ€: {current_state}\")\n",
    "    \n",
    "    if current_state in completed_states:\n",
    "        print(f\"\\nğŸ‰ Job å®Œæˆï¼æœ€ç»ˆçŠ¶æ€: {current_state}\")\n",
    "        break\n",
    "    \n",
    "    poll_count += 1\n",
    "    if poll_count < max_polls:\n",
    "        print(\"   ç­‰å¾… 5 ç§’åå†æ¬¡æ£€æŸ¥...\")\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  å·²è½®è¯¢ {max_polls} æ¬¡ï¼Œjob ä»åœ¨å¤„ç†ä¸­\")\n",
    "    print(\"   å®é™…ä½¿ç”¨æ—¶å¯ä»¥ä¿å­˜ job_nameï¼Œç¨åå†æ¥è·å–ç»“æœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03cb53",
   "metadata": {},
   "source": [
    "# è·å–ç»“æœ\n",
    "if batch_job.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "    print(\"ğŸ“¦ è·å– embedding ç»“æœ...\\n\")\n",
    "    \n",
    "    # å¯¹äº inline requestsï¼Œç»“æœåœ¨ dest.inlined_embed_content_responses ä¸­\n",
    "    if batch_job.dest and batch_job.dest.inlined_embed_content_responses:\n",
    "        responses = batch_job.dest.inlined_embed_content_responses\n",
    "        print(f\"âœ… æˆåŠŸè·å– {len(responses)} ä¸ª embeddingï¼\\n\")\n",
    "        \n",
    "        # å…ˆæŸ¥çœ‹ç¬¬ä¸€ä¸ªå“åº”çš„ç»“æ„\n",
    "        first_resp = responses[0]\n",
    "        print(f\"å“åº”ç±»å‹: {type(first_resp)}\")\n",
    "        print(f\"å“åº”å¯¹è±¡: {first_resp.response}\")\n",
    "        print(f\"å“åº”å†…éƒ¨ç±»å‹: {type(first_resp.response)}\")\n",
    "        print(f\"å“åº”å†…éƒ¨å±æ€§: {[attr for attr in dir(first_resp.response) if not attr.startswith('_')]}\")\n",
    "        print()\n",
    "        \n",
    "        # æ£€æŸ¥ response.embedding\n",
    "        if hasattr(first_resp.response, 'embedding'):\n",
    "            print(f\"embedding å±æ€§: {type(first_resp.response.embedding)}\")\n",
    "            print(f\"embedding å†…å®¹å±æ€§: {[attr for attr in dir(first_resp.response.embedding) if not attr.startswith('_')]}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  æ²¡æœ‰æ‰¾åˆ° inline responses\")\n",
    "else:\n",
    "    print(f\"âŒ Job æœªæˆåŠŸå®Œæˆï¼ŒçŠ¶æ€: {batch_job.state.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb28b79f",
   "metadata": {},
   "source": [
    "## æµ‹è¯•å°è£…çš„ Batch Embedding å‡½æ•°\n",
    "\n",
    "ç°åœ¨æµ‹è¯•æˆ‘ä»¬å°è£…å¥½çš„ `google_batch_embedding` å‡½æ•°ï¼Œå®ƒä½¿ç”¨ Google Batch APIï¼Œä»·æ ¼å‡åŠï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a06b8513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸå¯¼å…¥ google_batch_embedding å‡½æ•°\n"
     ]
    }
   ],
   "source": [
    "# é‡æ–°å¯¼å…¥ä»¥è·å–æœ€æ–°çš„ä»£ç \n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# å¦‚æœæ¨¡å—å·²ç»å¯¼å…¥ï¼Œé‡æ–°åŠ è½½\n",
    "if 'papersys.embedding' in sys.modules:\n",
    "    importlib.reload(sys.modules['papersys.embedding'])\n",
    "\n",
    "from papersys.embedding import google_batch_embedding\n",
    "\n",
    "print(\"âœ… æˆåŠŸå¯¼å…¥ google_batch_embedding å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3422737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡åµŒå…¥ 8 ä¸ªä¸­æ–‡æ–‡æœ¬\n",
      "\n",
      "æ–‡æœ¬åˆ—è¡¨:\n",
      "  1. æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "  2. Python æ˜¯æ•°æ®ç§‘å­¦çš„é¦–é€‰è¯­è¨€\n",
      "  3. è‡ªç„¶è¯­è¨€å¤„ç†ä½¿ AI èƒ½å¤Ÿç†è§£æ–‡æœ¬\n",
      "  4. æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\n",
      "  5. è¿ç§»å­¦ä¹ å¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´\n",
      "  6. ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘çš„å·¥ä½œæ–¹å¼\n",
      "  7. è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿçœ‹æ‡‚å›¾åƒ\n",
      "  8. å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æ¥è®­ç»ƒæ™ºèƒ½ä½“\n"
     ]
    }
   ],
   "source": [
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "test_texts = [\n",
    "    \"æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\",\n",
    "    \"Python æ˜¯æ•°æ®ç§‘å­¦çš„é¦–é€‰è¯­è¨€\",\n",
    "    \"è‡ªç„¶è¯­è¨€å¤„ç†ä½¿ AI èƒ½å¤Ÿç†è§£æ–‡æœ¬\",\n",
    "    \"æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\",\n",
    "    \"è¿ç§»å­¦ä¹ å¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´\",\n",
    "    \"ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘çš„å·¥ä½œæ–¹å¼\",\n",
    "    \"è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿçœ‹æ‡‚å›¾åƒ\",\n",
    "    \"å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æ¥è®­ç»ƒæ™ºèƒ½ä½“\"\n",
    "]\n",
    "\n",
    "print(f\"å‡†å¤‡åµŒå…¥ {len(test_texts)} ä¸ªä¸­æ–‡æ–‡æœ¬\")\n",
    "print(\"\\næ–‡æœ¬åˆ—è¡¨:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"  {i}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddbfbbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹è°ƒç”¨ Batch Embedding API...\n",
      "ğŸ’° æç¤ºï¼šä½¿ç”¨ Batch API ä»·æ ¼æ˜¯æ­£å¸¸ API çš„ 50%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyk/code/papersys/papersys/embedding.py:58: ExperimentalWarning: batches.create_embeddings() is experimental and may change without notice.\n",
      "  batch_job = client.batches.create_embeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸï¼è€—æ—¶: 61.87 ç§’\n",
      "\n",
      "è¿”å›çš„åµŒå…¥çŸ©é˜µå½¢çŠ¶: (8, 768)\n",
      "æ•°æ®ç±»å‹: float32\n",
      "æ¯ä¸ªå‘é‡çš„ç»´åº¦: 768\n"
     ]
    }
   ],
   "source": [
    "# è°ƒç”¨ google_batch_embedding å‡½æ•°\n",
    "import time\n",
    "\n",
    "print(\"ğŸš€ å¼€å§‹è°ƒç”¨ Batch Embedding API...\")\n",
    "print(\"ğŸ’° æç¤ºï¼šä½¿ç”¨ Batch API ä»·æ ¼æ˜¯æ­£å¸¸ API çš„ 50%\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# è°ƒç”¨å‡½æ•°ï¼Œä½¿ç”¨ 768 ç»´åº¦ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´\n",
    "embeddings_matrix = google_batch_embedding(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    inputs=test_texts,\n",
    "    task_type=\"RETRIEVAL_DOCUMENT\",\n",
    "    output_dimensionality=768\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… æˆåŠŸï¼è€—æ—¶: {elapsed_time:.2f} ç§’\\n\")\n",
    "print(f\"è¿”å›çš„åµŒå…¥çŸ©é˜µå½¢çŠ¶: {embeddings_matrix.shape}\")\n",
    "print(f\"æ•°æ®ç±»å‹: {embeddings_matrix.dtype}\")\n",
    "print(f\"æ¯ä¸ªå‘é‡çš„ç»´åº¦: {embeddings_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d1b926c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æŸ¥çœ‹å‰ä¸¤ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆå‰10ç»´ï¼‰:\n",
      "\n",
      "æ–‡æœ¬ 1: æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "Embedding: [-0.01820322  0.00460717  0.0146458  -0.08169461 -0.00463093  0.01815054\n",
      " -0.01603236  0.02238519  0.0202407   0.00680002]\n",
      "å‘é‡èŒƒæ•°: 0.5889\n",
      "\n",
      "æ–‡æœ¬ 2: Python æ˜¯æ•°æ®ç§‘å­¦çš„é¦–é€‰è¯­è¨€\n",
      "Embedding: [-0.0145923   0.00761684  0.00508905 -0.08106402 -0.01248688  0.01986415\n",
      " -0.00407611  0.00463162  0.02045821 -0.00548646]\n",
      "å‘é‡èŒƒæ•°: 0.5856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹éƒ¨åˆ†åµŒå…¥å‘é‡\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ“Š æŸ¥çœ‹å‰ä¸¤ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆå‰10ç»´ï¼‰:\\n\")\n",
    "for i in range(min(2, len(test_texts))):\n",
    "    print(f\"æ–‡æœ¬ {i+1}: {test_texts[i]}\")\n",
    "    print(f\"Embedding: {embeddings_matrix[i][:10]}\")\n",
    "    print(f\"å‘é‡èŒƒæ•°: {np.linalg.norm(embeddings_matrix[i]):.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aded55c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ:\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8102:\n",
      "  æ–‡æœ¬1: æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "  æ–‡æœ¬2: Python æ˜¯æ•°æ®ç§‘å­¦çš„é¦–é€‰è¯­è¨€\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8388:\n",
      "  æ–‡æœ¬1: æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "  æ–‡æœ¬2: è‡ªç„¶è¯­è¨€å¤„ç†ä½¿ AI èƒ½å¤Ÿç†è§£æ–‡æœ¬\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8366:\n",
      "  æ–‡æœ¬1: æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "  æ–‡æœ¬2: æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8143:\n",
      "  æ–‡æœ¬1: æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "  æ–‡æœ¬2: è¿ç§»å­¦ä¹ å¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8074:\n",
      "  æ–‡æœ¬1: æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "  æ–‡æœ¬2: ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘çš„å·¥ä½œæ–¹å¼\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8608:\n",
      "  æ–‡æœ¬1: æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "  æ–‡æœ¬2: è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿçœ‹æ‡‚å›¾åƒ\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8250:\n",
      "  æ–‡æœ¬1: æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "  æ–‡æœ¬2: å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æ¥è®­ç»ƒæ™ºèƒ½ä½“\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8219:\n",
      "  æ–‡æœ¬1: è‡ªç„¶è¯­è¨€å¤„ç†ä½¿ AI èƒ½å¤Ÿç†è§£æ–‡æœ¬\n",
      "  æ–‡æœ¬2: ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘çš„å·¥ä½œæ–¹å¼\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8551:\n",
      "  æ–‡æœ¬1: è‡ªç„¶è¯­è¨€å¤„ç†ä½¿ AI èƒ½å¤Ÿç†è§£æ–‡æœ¬\n",
      "  æ–‡æœ¬2: è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿçœ‹æ‡‚å›¾åƒ\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8035:\n",
      "  æ–‡æœ¬1: è‡ªç„¶è¯­è¨€å¤„ç†ä½¿ AI èƒ½å¤Ÿç†è§£æ–‡æœ¬\n",
      "  æ–‡æœ¬2: å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æ¥è®­ç»ƒæ™ºèƒ½ä½“\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8371:\n",
      "  æ–‡æœ¬1: æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\n",
      "  æ–‡æœ¬2: è¿ç§»å­¦ä¹ å¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8007:\n",
      "  æ–‡æœ¬1: æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\n",
      "  æ–‡æœ¬2: ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘çš„å·¥ä½œæ–¹å¼\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8264:\n",
      "  æ–‡æœ¬1: æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\n",
      "  æ–‡æœ¬2: è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿçœ‹æ‡‚å›¾åƒ\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8085:\n",
      "  æ–‡æœ¬1: æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\n",
      "  æ–‡æœ¬2: å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æ¥è®­ç»ƒæ™ºèƒ½ä½“\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8177:\n",
      "  æ–‡æœ¬1: ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘çš„å·¥ä½œæ–¹å¼\n",
      "  æ–‡æœ¬2: è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿçœ‹æ‡‚å›¾åƒ\n",
      "\n",
      "ç›¸ä¼¼åº¦ 0.8098:\n",
      "  æ–‡æœ¬1: è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿçœ‹æ‡‚å›¾åƒ\n",
      "  æ–‡æœ¬2: å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æ¥è®­ç»ƒæ™ºèƒ½ä½“\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# è®¡ç®—æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# å½’ä¸€åŒ–åµŒå…¥å‘é‡ï¼ˆå¯¹äº 768 ç»´åº¦éœ€è¦å½’ä¸€åŒ–ï¼‰\n",
    "normalized_embeddings = embeddings_matrix / np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "similarity_matrix = cosine_similarity(normalized_embeddings)\n",
    "\n",
    "print(\"ğŸ” æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ:\\n\")\n",
    "# æ‰¾å‡ºæœ€ç›¸ä¼¼çš„æ–‡æœ¬å¯¹\n",
    "for i in range(len(test_texts)):\n",
    "    for j in range(i + 1, len(test_texts)):\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        if similarity > 0.8:  # åªæ˜¾ç¤ºé«˜åº¦ç›¸ä¼¼çš„\n",
    "            print(f\"ç›¸ä¼¼åº¦ {similarity:.4f}:\")\n",
    "            print(f\"  æ–‡æœ¬1: {test_texts[i]}\")\n",
    "            print(f\"  æ–‡æœ¬2: {test_texts[j]}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "787ff2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æŸ¥è¯¢: 'AI å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyk/code/papersys/papersys/embedding.py:58: ExperimentalWarning: batches.create_embeddings() is experimental and may change without notice.\n",
      "  batch_job = client.batches.create_embeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æ¡£:\n",
      "\n",
      "1. [ç›¸ä¼¼åº¦: 0.7143] æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ\n",
      "2. [ç›¸ä¼¼åº¦: 0.6679] è‡ªç„¶è¯­è¨€å¤„ç†ä½¿ AI èƒ½å¤Ÿç†è§£æ–‡æœ¬\n",
      "3. [ç›¸ä¼¼åº¦: 0.6455] æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\n"
     ]
    }
   ],
   "source": [
    "# æ¼”ç¤ºæŸ¥è¯¢åŠŸèƒ½\n",
    "query_text = \"AI å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»\"\n",
    "print(f\"ğŸ” æŸ¥è¯¢: '{query_text}'\\n\")\n",
    "\n",
    "# è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡ï¼ˆä½¿ç”¨ RETRIEVAL_QUERY ä»»åŠ¡ç±»å‹ï¼‰\n",
    "query_embedding = google_batch_embedding(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    inputs=[query_text],\n",
    "    task_type=\"RETRIEVAL_QUERY\",  # æŸ¥è¯¢ä½¿ç”¨ä¸åŒçš„ä»»åŠ¡ç±»å‹\n",
    "    output_dimensionality=768\n",
    ")\n",
    "\n",
    "# å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡\n",
    "query_embedding_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "# è®¡ç®—ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦\n",
    "similarities = cosine_similarity(query_embedding_norm, normalized_embeddings)[0]\n",
    "\n",
    "# æ’åºå¹¶æ˜¾ç¤ºæœ€ç›¸å…³çš„æ–‡æ¡£\n",
    "top_k = 3\n",
    "top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "print(f\"æœ€ç›¸å…³çš„ {top_k} ä¸ªæ–‡æ¡£:\\n\")\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{rank}. [ç›¸ä¼¼åº¦: {similarities[idx]:.4f}] {test_texts[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a484ad0",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "### âœ… æˆåŠŸå®ç°çš„åŠŸèƒ½\n",
    "\n",
    "1. **å°è£…äº† `google_batch_embedding` å‡½æ•°**\n",
    "   - ä½ç½®ï¼š`papersys/embedding.py`\n",
    "   - ä½¿ç”¨ Google Gemini Batch Embedding API\n",
    "   - æ”¯æŒè‡ªå®šä¹‰ä»»åŠ¡ç±»å‹å’Œè¾“å‡ºç»´åº¦\n",
    "   - è‡ªåŠ¨å¤„ç†å¼‚æ­¥ç­‰å¾…å’Œé”™è¯¯å¤„ç†\n",
    "\n",
    "2. **å…³é”®ä¼˜åŠ¿**\n",
    "   - ğŸ’° **ä»·æ ¼å‡åŠ**ï¼šBatch API ä»·æ ¼æ˜¯æ­£å¸¸ API çš„ 50%\n",
    "   - ğŸš€ **æ‰¹é‡å¤„ç†**ï¼šä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ–‡æœ¬\n",
    "   - ğŸ¯ **ä»»åŠ¡ä¼˜åŒ–**ï¼šæ”¯æŒå¤šç§ä»»åŠ¡ç±»å‹ï¼ˆæ£€ç´¢ã€åˆ†ç±»ã€èšç±»ç­‰ï¼‰\n",
    "   - ğŸ“¦ **ä¾¿æ·ä½¿ç”¨**ï¼šè¿”å›æ ‡å‡†çš„ numpy æ•°ç»„ï¼Œæ˜“äºé›†æˆ\n",
    "\n",
    "3. **æµ‹è¯•ç»“æœ**\n",
    "   - âœ… æˆåŠŸåµŒå…¥ 8 ä¸ªä¸­æ–‡æ–‡æœ¬\n",
    "   - âœ… ç”Ÿæˆ 768 ç»´åµŒå…¥å‘é‡\n",
    "   - âœ… è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦æ­£å¸¸\n",
    "   - âœ… è¯­ä¹‰æœç´¢åŠŸèƒ½æ­£å¸¸\n",
    "\n",
    "### ğŸ“ ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "from papersys.embedding import google_batch_embedding\n",
    "\n",
    "# è·å–æ–‡æ¡£åµŒå…¥\n",
    "embeddings = google_batch_embedding(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    inputs=[\"æ–‡æœ¬1\", \"æ–‡æœ¬2\", \"æ–‡æœ¬3\"],\n",
    "    task_type=\"RETRIEVAL_DOCUMENT\",  # æ–‡æ¡£æ£€ç´¢\n",
    "    output_dimensionality=768  # å¯é€‰ï¼š768, 1536, 3072\n",
    ")\n",
    "\n",
    "# è·å–æŸ¥è¯¢åµŒå…¥\n",
    "query_embedding = google_batch_embedding(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    inputs=[\"æŸ¥è¯¢æ–‡æœ¬\"],\n",
    "    task_type=\"RETRIEVAL_QUERY\",  # æŸ¥è¯¢æ£€ç´¢\n",
    "    output_dimensionality=768\n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸ’¡ å»ºè®®\n",
    "\n",
    "1. **ç»´åº¦é€‰æ‹©**ï¼šä½¿ç”¨ 768 ç»´å¯ä»¥èŠ‚çœ 75% çš„å­˜å‚¨ç©ºé—´ï¼Œæ€§èƒ½æŸå¤±å¾ˆå°\n",
    "2. **ä»»åŠ¡ç±»å‹**ï¼šæ–‡æ¡£ä½¿ç”¨ `RETRIEVAL_DOCUMENT`ï¼ŒæŸ¥è¯¢ä½¿ç”¨ `RETRIEVAL_QUERY`\n",
    "3. **å½’ä¸€åŒ–**ï¼šå¯¹äºé 3072 ç»´çš„åµŒå…¥ï¼Œéœ€è¦æ‰‹åŠ¨å½’ä¸€åŒ–åå†è®¡ç®—ç›¸ä¼¼åº¦\n",
    "4. **æ‰¹é‡å¤§å°**ï¼šå•æ¬¡å¯å¤„ç†å¤§é‡æ–‡æœ¬ï¼Œå»ºè®®æ ¹æ®å®é™…æƒ…å†µåˆ†æ‰¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1587a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## é™„å½•ï¼šAPI å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | æ™®é€š Embedding API | Batch Embedding API |\n",
    "|------|-------------------|---------------------|\n",
    "| **ä»·æ ¼** | æ ‡å‡†ä»·æ ¼ | **50% æŠ˜æ‰£** ğŸ’° |\n",
    "| **å¤„ç†æ–¹å¼** | åŒæ­¥ï¼Œç«‹å³è¿”å› | å¼‚æ­¥ï¼Œç­‰å¾…å®Œæˆ |\n",
    "| **å“åº”æ—¶é—´** | æ¯«ç§’çº§ | å‡ ç§’åˆ°å‡ åˆ†é’Ÿ |\n",
    "| **é€‚ç”¨åœºæ™¯** | å®æ—¶æŸ¥è¯¢ | æ‰¹é‡é¢„å¤„ç† |\n",
    "| **æœ€å¤§æ‰¹é‡** | æœ‰é™åˆ¶ | æ”¯æŒå¤§é‡æ–‡æœ¬ |\n",
    "| **æ¨èç”¨é€”** | åœ¨çº¿æœç´¢ã€å®æ—¶åº”ç”¨ | æ•°æ®åº“æ„å»ºã€ç¦»çº¿å¤„ç† |\n",
    "\n",
    "### ä½•æ—¶ä½¿ç”¨ Batch APIï¼Ÿ\n",
    "\n",
    "âœ… **é€‚åˆä½¿ç”¨ Batch API**ï¼š\n",
    "- æ„å»ºå‘é‡æ•°æ®åº“\n",
    "- é¢„å¤„ç†å¤§é‡æ–‡æ¡£\n",
    "- ç¦»çº¿æ•°æ®åˆ†æ\n",
    "- æˆæœ¬æ•æ„Ÿçš„åº”ç”¨\n",
    "\n",
    "âŒ **ä¸é€‚åˆä½¿ç”¨ Batch API**ï¼š\n",
    "- å®æ—¶ç”¨æˆ·æŸ¥è¯¢\n",
    "- éœ€è¦ç«‹å³å“åº”çš„åœºæ™¯\n",
    "- å•ä¸ªæ–‡æœ¬çš„åµŒå…¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papersys (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
